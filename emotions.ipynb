{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.microsoft.com/en-us/azure/cognitive-services/face/quickstarts/client-libraries?pivots=programming-language-python&tabs=windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Info\n",
    "\n",
    "face_client.face.detect_with_url(url, return_face_id=True, return_face_landmarks=False, return_face_attributes=None, recognition_model='recognition_01', return_recognition_model=False, detection_model='detection_01', custom_headers=None, raw=False, **operation_config)\n",
    "\n",
    "Detect human faces in an image, return face rectangles, and optionally with faceIds, landmarks, and attributes.\n",
    "\n",
    "No image will be stored. Only the extracted face feature will be\n",
    "stored on server. The faceId is an identifier of the face feature and will be used in Face - Identify, Face - Verify, and Face - Find Similar. The stored face feature(s) will expire and be deleted 24 hours after the original detection call.\n",
    "\n",
    "Optional parameters include faceId, landmarks, and attributes.\n",
    "Attributes include age, gender, headPose, smile, facialHair, glasses, emotion, hair, makeup, occlusion, accessories, blur, exposure and noise. Some of the results returned for specific attributes may not be highly accurate.\n",
    "\n",
    "JPEG, PNG, GIF (the first frame), and BMP format are supported. The\n",
    "allowed image file size is from 1KB to 6MB.\n",
    "\n",
    "Up to 100 faces can be returned for an image. Faces are ranked by\n",
    "face rectangle size from large to small.\n",
    "\n",
    "For optimal results when querying Face -\n",
    "Identify, Face - Verify, and Face - Find Similar ('returnFaceId' is true), please use faces that are: frontal, clear, and with a minimum size of 200x200 pixels (100 pixels between eyes).\n",
    "\n",
    "The minimum detectable face size is 36x36 pixels in an image no\n",
    "larger than 1920x1080 pixels. Images with dimensions higher than\n",
    "\n",
    "1920x1080 pixels will need a proportionally larger minimum face size.\n",
    "\n",
    "Different 'detectionModel' values can be provided. To use and compare\n",
    "different detection models, please refer to How to specify a detection model | Model | Recommended use-case(s) | | ---------- | -------- | | 'detection_01': | The default detection model for Face - Detect. Recommend for near frontal face detection. For scenarios with exceptionally large angle (head-pose) faces, occluded faces or wrong image orientation, the faces in such cases may not be detected. | | 'detection_02': | Detection model released in 2019 May with improved accuracy especially on small, side and blurry faces. |\n",
    "\n",
    "Different 'recognitionModel' values are provided. If follow-up\n",
    "operations like Verify, Identify, Find Similar are needed, please specify the recognition model with 'recognitionModel' parameter. The default value for 'recognitionModel' is 'recognition_01', if latest model needed, please explicitly specify the model you need in this parameter. Once specified, the detected faceIds will be associated with the specified recognition model. More details, please refer to How to specify a recognition model | Model | Recommended use-case(s) | | ---------- | -------- | | 'recognition_01': | The default recognition model for Face - Detect. All those faceIds created before 2019 March are bonded with this recognition model. | | 'recognition_02': | Recognition model released in 2019 March. 'recognition_02' is recommended since its overall accuracy is improved compared with 'recognition_01'. |.\n",
    "\n",
    ":param url: Publicly reachable URL of an image :type url: str :param return_face_id: A value indicating whether the operation should\n",
    "\n",
    "return faceIds of detected faces.\n",
    ":type return_face_id: bool :param return_face_landmarks: A value indicating whether the operation\n",
    "\n",
    "should return landmarks of the detected faces.\n",
    ":type return_face_landmarks: bool :param return_face_attributes: Analyze and return the one or more\n",
    "\n",
    "specified face attributes in the comma-separated string like\n",
    "\"returnFaceAttributes=age,gender\". Supported face attributes include\n",
    "age, gender, headPose, smile, facialHair, glasses and emotion. Note\n",
    "that each face attribute analysis has additional computational and\n",
    "time cost.\n",
    ":type return_face_attributes: list[str or\n",
    "\n",
    "~azure.cognitiveservices.vision.face.models.FaceAttributeType]\n",
    ":param recognition_model: Name of recognition model. Recognition model\n",
    "\n",
    "is used when the face features are extracted and associated with\n",
    "detected faceIds, (Large)FaceList or (Large)PersonGroup. A recognition\n",
    "model name can be provided when performing Face - Detect or\n",
    "(Large)FaceList - Create or (Large)PersonGroup - Create. The default\n",
    "value is 'recognition_01', if latest model needed, please explicitly\n",
    "specify the model you need. Possible values include: 'recognition_01',\n",
    "'recognition_02'\n",
    ":type recognition_model: str or\n",
    "\n",
    "~azure.cognitiveservices.vision.face.models.RecognitionModel\n",
    ":param return_recognition_model: A value indicating whether the\n",
    "\n",
    "operation should return 'recognitionModel' in response.\n",
    ":type return_recognition_model: bool :param detection_model: Name of detection model. Detection model is\n",
    "\n",
    "used to detect faces in the submitted image. A detection model name\n",
    "can be provided when performing Face - Detect or (Large)FaceList - Add\n",
    "Face or (Large)PersonGroup - Add Face. The default value is\n",
    "'detection_01', if another model is needed, please explicitly specify\n",
    "it. Possible values include: 'detection_01', 'detection_02'\n",
    ":type detection_model: str or\n",
    "\n",
    "~azure.cognitiveservices.vision.face.models.DetectionModel\n",
    ":param dict custom_headers: headers that will be added to the request :param bool raw: returns the direct response alongside the\n",
    "\n",
    "deserialized response\n",
    ":param operation_config: :ref:`Operation configuration\n",
    "\n",
    "overrides msrest:optionsforoperations `.\n",
    ":return: list or ClientRawResponse if raw=true :rtype: list[~azure.cognitiveservices.vision.face.models.DetectedFace]\n",
    "\n",
    "or ~msrest.pipeline.ClientRawResponse\n",
    ":raises:\n",
    "\n",
    ":class:APIErrorException azure.cognitiveservices.vision.face.models.APIErrorException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download if necessary\n",
    "# !pip install cognitive_face\n",
    "# !pip install azure.cognitiveservices.vision.face\n",
    "\n",
    "# install libraries\n",
    "import asyncio\n",
    "import io\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageDraw\n",
    "from azure.cognitiveservices.vision.face import FaceClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "from azure.cognitiveservices.vision.face.models import TrainingStatusType, Person, SnapshotObjectType, OperationStatusType\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the FACE_SUBSCRIPTION_KEY environment variable with your key as the value.\n",
    "# This key will serve all examples in this document.\n",
    "KEY = os.environ['FACE_SUBSCRIPTION_KEY']\n",
    "\n",
    "# Set the FACE_ENDPOINT environment variable with the endpoint from your Face service in Azure.\n",
    "# This endpoint will be used in all examples in this quickstart.\n",
    "ENDPOINT = os.environ['FACE_ENDPOINT']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful Documentation\n",
    "\n",
    "Packages, models, operations, version, Classes (FaceClient, FaceClientConfiguration) - https://docs.microsoft.com/en-us/python/api/azure-cognitiveservices-vision-face/azure.cognitiveservices.vision.face?view=azure-python *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an authenticated FaceClient.\n",
    "face_client = FaceClient(ENDPOINT, CognitiveServicesCredentials(KEY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Example \n",
    "### Detect faces in an image\n",
    "\n",
    "The following code detects a face in a remote image. It prints the detected face's ID to the console and also stores it in program memory. Then, it detects the faces in an image with multiple people and prints their IDs to the console as well. By changing the parameters in the detect_with_url method, you can return different information with each DetectedFace object.\n",
    "\n",
    "See the sample code on GitHub for more detection scenarios.\n",
    "\n",
    "https://github.com/Azure-Samples/cognitive-services-quickstart-code/blob/master/python/Face/FaceQuickstart.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Detected face ID from john-f-kennedy---mini-biography.jpg :\n1517cb77-12c1-4fc6-bd55-7e8db79816ee\n\n"
    }
   ],
   "source": [
    "# Detect a face in an image that contains a single face\n",
    "single_face_image_url = 'https://www.biography.com/.image/t_share/MTQ1MzAyNzYzOTgxNTE0NTEz/john-f-kennedy---mini-biography.jpg'\n",
    "single_image_name = os.path.basename(single_face_image_url)\n",
    "detected_faces = face_client.face.detect_with_url(url=single_face_image_url)\n",
    "if not detected_faces:\n",
    "    raise Exception('No face detected from image {}'.format(single_image_name))\n",
    "\n",
    "# Display the detected face ID in the first single-face image.\n",
    "# Face IDs are used for comparison to faces (their IDs) detected in other images.\n",
    "print('Detected face ID from', single_image_name, ':')\n",
    "for face in detected_faces: print (face.face_id)\n",
    "print()\n",
    "\n",
    "# Save this ID for use in Find Similar\n",
    "first_image_face_ID = detected_faces[0].face_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Example\n",
    "### Display and frame facesÂ¶\n",
    "The following code outputs the given image to the display and draws rectangles around the faces, using the DetectedFace.faceRectangle property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Drawing rectangle around face... see popup for results.\n"
    }
   ],
   "source": [
    "# Detect a face in an image that contains a single face\n",
    "single_face_image_url = 'https://raw.githubusercontent.com/Microsoft/Cognitive-Face-Windows/master/Data/detection1.jpg'\n",
    "single_image_name = os.path.basename(single_face_image_url)\n",
    "detected_faces = face_client.face.detect_with_url(url=single_face_image_url)\n",
    "if not detected_faces:\n",
    "    raise Exception('No face detected from image {}'.format(single_image_name))\n",
    "\n",
    "# Convert width height to a point in a rectangle\n",
    "def getRectangle(faceDictionary):\n",
    "    rect = faceDictionary.face_rectangle\n",
    "    left = rect.left\n",
    "    top = rect.top\n",
    "    right = left + rect.width\n",
    "    bottom = top + rect.height\n",
    "    \n",
    "    return ((left, top), (right, bottom))\n",
    "\n",
    "\n",
    "# Download the image from the url\n",
    "response = requests.get(single_face_image_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "# For each face returned use the face rectangle and draw a red box.\n",
    "print('Drawing rectangle around face... see popup for results.')\n",
    "draw = ImageDraw.Draw(img)\n",
    "for face in detected_faces:\n",
    "    draw.rectangle(getRectangle(face), outline='red')\n",
    "\n",
    "# Display the image in the users default image browser.\n",
    "img.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test getting emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'additional_properties': {}, 'anger': 0.0, 'contempt': 0.002, 'disgust': 0.0, 'fear': 0.0, 'happiness': 0.0, 'neutral': 0.997, 'sadness': 0.001, 'surprise': 0.0}\n"
    }
   ],
   "source": [
    "# face_client.face.detect_with_url()\n",
    "\n",
    "test = face_client.face.detect_with_url(url=single_face_image_url, return_face_id=True, return_face_landmarks=True, return_face_attributes=['age', 'gender', 'headPose', 'smile', 'facialHair', 'glasses', 'emotion', 'hair', 'makeup', 'occlusion', 'accessories', 'blur', 'exposure', 'noise'], recognition_model='recognition_01', return_recognition_model=True, detection_model='detection_01', custom_headers=None, raw=False)\n",
    "\n",
    "# face_client.face.models.Emotion()\n",
    "for face in test:\n",
    "  print(face.face_attributes.emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'emotion_pics'\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}